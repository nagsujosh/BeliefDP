# Transformer backbone configuration
backbone_type: transformer
d_model: 256
nhead: 4
num_layers: 2               # 2 layers (was 4) — less capacity to reduce overfitting
dim_feedforward: 256         # Narrower FFN (was 512) — matches d_model
dropout: 0.3                 # Higher dropout (was 0.1)
max_seq_len: 512

# Temperature scaling for phase logits
temperature: 1.0             # Learnable or fixed; calibrates softmax logits
learn_temperature: true
